\documentclass[a4page, 12pt]{article}


\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[margins=1.2in]{geometry}

\usepackage{titleps}    % barra sopra
\usepackage{scrextend}  % check odd
\def\printpage{\thepage}
\def\printleft{\Ifthispageodd{\sectiontitle}{\printpage}}
\def\printright{\Ifthispageodd{\printpage}{High Dimentional Data Analysis: L'algoritmo LARS}}
\newpagestyle{main}{\setheadrule{0.3pt}\sethead{\printleft}{}{\printright}}
\pagestyle{main}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tcolorbox}
\usepackage{graphicx}

\usepackage{minted}
\usemintedstyle{manni}

\title{
    \textbf{High Dimentional Data Analysis} \\
    \large{L'algoritmo LARS}}

\author{
    Riccardo Cervero, 794126 \and
    Federico Moiraghi, 799735 \and
    Marco Ferrario, 795203
}

\date{}

\begin{document}
\maketitle

\begin{tcolorbox}[colback=white, colframe=gray, boxrule=0.1mm, sharp corners=all]
\begin{abstract}
    Gli obiettivi principali di questo progetto sono la presentazione e l'applicazione di un metodo in grado di eseguire stime di regressione efficaci anche quando la dimensionalità dei dati è molto superiore al numero di osservazioni: l'algoritmo \textit{Least-Angle Regression} (LARS). In particolare, dopo aver introdotto le sue caratteristiche, verranno eseguite due simulazioni Montecarlo - rispettivamente di tipo \textit{Fixed-X setting} e \textit{Random-X setting} - col fine di analizzare la scomposizione dell'errore di prevesione. Infine, la metodologia sarà testata su un caso di studio reale riguardante il campo della bioinformatica. In entrambe le applicazioni, i risultati dell'algoritmo LARS verranno confrontati con quelli prodotti da una regressione \textit{Ridge}.
\end{abstract}
\end{tcolorbox}
\thispagestyle{empty}
\newpage
\tableofcontents
% \thispagestyle{empty}
\newpage

\section{Tecniche di \textit{shrinkage}}  % RCrvro
Nell'epoca dei \textit{Big Data}, in cui sono facilmente disponibili grandi quantità di dati per diversi casi applicativi, la classica analisi di regressione, basata sulla stima dei minimi quadrati ordinari dei coefficienti, fatica nella costruzione di un modello lineare in grado di descrivere efficacemente la relazione tra la variabile $y$ e le covariate $X$, e produce una stima dell'errore poco affidabile.
In particolare, quando la dimensionalità dei dati $p$ è maggiore della loro cardinalità $n$, sorgono alcuni problemi che il metodo OLS non è in grado di gestire in modo appropriato.
La prima conseguenza consiste in un crescente rischio di multicollinearità tra le covariate, che impedirebbe l'ottenimento di risultati corretti dalla regressione, sovrastimando gli indici di bontà di adattamento e sottostimando la significatività statistica dei singoli coefficienti.
In secondo luogo, la presenza di troppe colonne potrebbe causare l'evento di \textit{overfitting}, che si verifica quando l'iperpiano di regressione non è in grado di catturare il segnale principale, poiché eccessivamente trasportato dal rumore da cui il \textit{training set} è caratterizzato.
In questa situazione, il modello avrà una generalizzazione bassa e produrrà un errore elevato su nuovi dati.
Inoltre, nel caso estremo di dati ad alta dimensionalità, cioè quando $p \gg n$, s'incorre nel grave problema di supercollinearità: il rango della matrice del disegno è inferiore a $p$, quindi il prodotto incrociato $X'X$ sarà un matrice singolare, e non potrà quindi essere invertito, rendendo impossibile una stima unica dei parametri di regressione.
Per i motivi sopra citati, è più ragionevole scegliere un modello parsimonioso, che dipenda ovvero da un minor numero di variabili esplicative e, di conseguenza, sia anche più interpretabile.
A tal proposito, sono state proposte varie metodologie utili al fine di ridurre la dimensionalità del modello, tra cui le cosiddette tecniche di "shrinkage", le quali consentono di contrarre i valori dei coefficienti verso lo zero attraverso una penalità, per diminuire la variabilità della funzione di regressione.
Nel caso in cui alcune stime finiscano per essere annullate, il metodo avrà effettuato un'implicita \textit{feature selection}.
Tra le tecniche di "shrinkage", verranno presentate la regressione \textit{Ridge} e l'algoritmo \textit{Least-Angle Regression} (LARS).

\subsection{Regressione \textit{Ridge}}
\label{ridge}
La supercollinearità causata dall'alta dimensionalità può essere risolta con un metodo ideato da Hoerl e Kennard: il prodotto $X' X$ è reso invertibile grazie all'aggiunta di un componente di penalità $\lambda I_p$ (con $\lambda \ge 0$), ottenendo un nuovo stimatore dei coefficienti di regressione $\hat{\beta}$, definito \textit{stimatore Ridge}:
$$\hat{\beta}^{\lambda}=(X'X+\lambda I_p)^{-1}X'y$$
dove $\lambda$ è il parametro di regolarizzazione pari alla norma $L_2$, tale per cui:
\begin{itemize}
    \item se $\lambda=0$, la soluzione coincide con il quella derivante dal metodo ai minimi quadrati ordinari;
    \item se $\lambda>0$, si produce una penalizzazione sul punto di minimo della funzione di perdita $D_{Ridge}^{(\beta,\lambda)}$.
\end{itemize}
Il valore atteso $\mathbb{E}[\hat{\beta}^{\lambda}]$ si dimostra essere pari a $(X'X+\lambda I_p)^{-1}(X'X)\beta\neq\beta$, dimostrando quindi che $\hat{\beta}^{\lambda}$ è distorto.
Invece, la varianza dello stimatore $Var(\hat{\beta}^{\lambda})$, oltre che dal parametro di \textit{tuning}, dipende dall'errore irriducibile $\sigma^2$, dall'inversa del prodotto incrociato della matrice del disegno e da un operatore lineare $w^{\lambda}=(X'X+\lambda I_p)^{-1}X'X$: $Var(\hat{\beta}^{\lambda})=\sigma^2w^{\lambda}(X'X)^{-1}(w^{\lambda})'$.\\
Per quanto riguarda i valori stimati, poichè il vettore delle risposte $\hat{y}^{\lambda}$ sarà pari a
$X(X'X+\lambda I_p)^{-1}X'y$, i residui non saranno più ortogonali a $\hat{y}^{\lambda}$.\\
Infine, è possibile dimostrare come questa tecnica di \textit{shrinkage}, interpretabile come un problema di penalizzazione, o, in alternativa, di ottimizzazione vincolata, oltre ad avere l'effetto di risolvere efficacemente la supercollinearità fra i regressori, riesca a produrre, per un'opportuna scelta di $\lambda$, un errore quadratico medio dello stimatore minore di quello offerto dal metodo \textit{OLS}.

\subsection{Algoritmo \textit{LARS}}
Nonostante sia un'ottima soluzione per contrarre la varianza della funzione di regressione, il metodo \textit{Ridge} è, in realtà, raramente in grado di effettuare una vera e propria \textit{feature selection}, ovvero di annullare i coefficienti che moltiplicano le variabili meno rilevanti per descrivere il fenomeno di studio.
Questa limitazione deriva dalla scelta di una norma $L_2$ come componente di penalizzazione. Essendo questa pari alla somma dei quadrati degli stimatori $\|\beta\|_2^2=\sum^P_{j=1}|\beta_j^2|$, si configura come una funzione lineare in $y$, dunque derivabile.
Pertanto, considerando l'esempio più semplice di una regressione su due covariate $X_1$ e $X_2$, se si ipotizzasse di rappresentare graficamente la soluzione al problema di minimizzazione vincolata, più o meno rigidamente a seconda del valore di $\lambda$, dalla penalizzazione della norma, si osserverebbe che il vettore composto dalle stime dei due coefficienti si troverebbe in corrispondenza del punto di tangenza fra il confine della regione ammissibile e la curva di livello dell'errore quadratico medio.
Tuttavia, avendo scelto la norma $L_2$ come vincolo, la regione ammissibile apparirà come un'area circolare attorno all'origine.
Come conseguenza di ciò, la soluzione \textit{Ridge} riuscirà a contrarre il valore di uno dei due stimatori vicino allo zero, ma raramente potrà annullarlo, diminuendo l'influenza ma mantenendo attiva la data covariata.
Questa problematica è ben visibile nella Figura \ref{fig:ridge-representation}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{ridge.png}
    \caption{Soluzione grafica offerta dal metodo \textit{Ridge} al problema di minimizzazione vincolata: l'area circolare è la regione ammissibile legata al vincolo della somma dei coefficienti quadrati. Le curve rosse sono interpretabili come le curve di livello dell'errore dal vettore dei coefficienti reali $\hat{\beta}$.}
    \label{fig:ridge-representation}
\end{figure}

Nasce pertanto la necessità di adottare metodologie che producano una soluzione sparsa, cioè in grado di fare una selezione continua del modello più parsimonioso nei limiti imposti dalla minimizzazione dell'errore quadratico medio.

L'obiettivo viene raggiunto sostituendo la norma $L_1$ alla norma $L_2$ come componente di penalizzazione regolata dal parametro di \textit{tuning} $\lambda$.
Essendo pari alla somma dei valori assoluti dei coefficienti $\|\beta\|_1=\sum^P_{j=1}|\beta_j|$, la norma $L_1$ è una funzione non lineare in $y$, pertanto non derivabile.
Ipotizzando la medesima situazione dell'esempio precedente, la nuova soluzione di regressione sarà ancora rappresentata dal punto di tangenza fra la curva di livello dell'errore e il confine della regione ammissibile.
Tuttavia, l'area disegnata dal vincolo della norma assoluta non apparirà più come una circonferenza, bensì come un rombo attorno all'origine.
In questo caso, il punto ottimo non potrà essere il frutto di alcun compromesso fra $X_1$ e $X_2$, ma si troverà necessariamente su uno dei due assi, annullando il valore di stima dell'altro, ed effettuando perciò una vera selezione del modello.
Le caratteristiche della soluzione con norma $L_1$ sono ben mostrate dalla Figura \ref{fig:lasso-representation}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{l1.png}
    \caption{Soluzione grafica offerta da una penalizzazione $L_1$ al problema di minimizzazione vincolata.}
    \label{fig:lasso-representation}
\end{figure}
Tale operazione non effettuerà una rotazione del vettore dei coefficienti, che corrispondeva - nel caso limitato della regressione \textit{Ridge} - a una sola contrazione dei valori, bensì una traslazione, ottenendo una stima sparsa.

Tra i metodi che sfruttano la norma $L_1$, il più popolare è sicuramente il metodo \textit{LASSO}, di cui esistono alcune varianti, tra cui l'algoritmo \textit{Least-Angle Regression (LARS)}.
Il \textit{LARS} può essere anche considerato come una versione del metodo di selezione del modello chiamato \textit{Forward Selection} - o, anche, \textit{Forward Stepwise Regression} -, il quale viene descritto da \textit{Weisberg}\cite{A} come segue: data una collezione di possibili predittori, si seleziona quello avente maggior valore assoluto di correlazione con la variabile risposta $y$, nominato $x_{j1}$, e si esegue una regressione lineare semplice di $y$ su $x_{j1}$.
Questa operazione produce un vettore dei residui ortogonale rispetto a $x_{j1}$, il quale viene a sua volta considerato come variabile risposta nello step successivo.
Ciò significa che gli altri candidati vengono proiettati ortogonalmente a $x_{j1}$, ripetendo quindi il processo di selezione del secondo predittore maggiormente correlato con $x_{j1}$.
Dopo $k$ step, si otterrà un insieme di variabili esplicative $x_{j1},x_{j2}, \dots ,x_{jk}$, che verrà infine utilizzato per costruire il modello lineare $k$-dimensionale finale.
La \textit{Forward Selection} viene definita come una tecnica di modellazione \textit{greedy}, poiché, ad esempio, in corrispondenza del secondo step potrebbe eliminare predittori rilevanti e altamente correlati con $x_j1$.
L'alternativa a questo metodo superficiale è denominata \textit{Forward Stagewise}, un procedimento da un lato molto più prudente - in quanto considera molteplici correlazioni - , ma dall'altro eccessivamente costoso, richiedendo un numero elevato di piccoli step prima di convergere al modello finale.
L'algoritmo \textit{LARS} nasce come un compromesso fra le due metodologie: grazie ad una formulazione semplice, esso compie step meno ampi dell'avida \textit{Forward Selection}, ma comunque non brevi come quelli della \textit{Forward Stagewise}, riuscendo al contempo sia ad acquisire più precisione sia a ridurre il peso computazionale di almeno un ordine di grandezza.
Infatti, grazie a questa accelerazione della computazionale, è dimostrato che l'algoritmo richieda al più $p$ iterazioni per ottenere il set completo delle soluzioni, dove $p$ è il numero di covariate.
Al contempo, esso mantiene la proprietà di parsimonia desiderata: solo un sotto-insieme degli stimatori $\hat{\beta_j}$ è non nullo.
Essendo strettamente connesso al metodo \textit{LASSO}, tramite le iterazioni appena spiegate, esso fornisce, in maniera estremamente efficiente, l'intera \textit{Lasso path solution}.

Il nome \textit{Least-Angle Regression} deriva proprio dalla geometria dell'algoritmo, che può essere riassunta in maniera più precisa nelle seguenti fasi:
\begin{enumerate}
    \item vengono inizializzati a zero i valori di tutti i coefficienti di regressione;
    \item viene trovato il candidato più correlato con la variabile risposta $y$, denominato, per esempio, $x_{j1}$;
    \item il vettore dei coefficienti subisce lo spostamento più lungo possibile in direzione del predittore $x_{j1}$, fin quando un altro candidato, denominato $x_{j2}$, ha almeno la stessa correlazione con il residuo attuale, ovvero derivato dalla regressione $y\sim x_{j1}$;
    \item a questo punto, anziché considerare $x_{j1}$ come variabile risposta della nuova iterazione - come effettuato dalla \textit{Forward Selection} -, si procede in direzione equiangolare fra i due predittori $x_{j1}$ e $x_{j2}$, fin quando anche un terzo candidato $x_{j3}$ ha almeno la stessa correlazione con il residuo attuale degli altri due;
    \item l'algoritmo continua a procedere in direzione equiangolare fra le variabili man mano selezionate - in questo caso $x_{j1}$ e $x_{j2}$ - e la nuova aggiunta - qui $x_{j3}$ -, seguendo quella che viene perciò definita direzione \textit{least-angle}, fin quando un ulteriore candidato acquisisce il requisito per entrare a far parte del modello finale. 
\end{enumerate}
La parsimonia del metodo risulta nel seguente meccanismo: poichè ad ogni iterazione viene aggiunta una covariata al modello, allora dopo $k$ step - con $k$ fissato -, solo $p-k$ coefficienti avranno valore nullo.

La strategia equiangolare può essere rappresentata in Figura \ref{fig:lars-strategy}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{equiang.png}
    \caption{L'algoritmo \textit{LARS} nel caso in cui il numero di covariate sia $p=2$. La linea blu riproduce il movimento dell'algoritmo \textit{LARS}, mentre la linea verde disegna la bisettrice fra gli assi relativi ai candidati considerati. La linea "a scaletta" segue i piccoli spostamenti prodotti da un ipotetico approccio \textit{Forward Stagewise}.}
    \label{fig:lars-strategy}
\end{figure}

Si ipotizza il caso più semplice in cui $p=2$, e si definiscono:
\begin{itemize}
    \item $\hat{\mu}=X\hat{\beta}$ come il vettore delle stime di $y$,
    \item $\Bar{y}_2$ come la proiezione di $y$ nello spazio lineare $\mathcal{L}(X_1,X_2)$.
\end{itemize}
Il vettore delle stime viene inizializzato nel punto $\hat{\mu}=0$. Nella situazione presentata, il vettore residuale $\Bar{y}_2-\hat{\mu}_0$ ha una maggior correlazione con il candidato $x_1$, essendo più vicino al relativo asse. Il successivo vettore di stima \textit{LARS} sarà dunque $\hat{\mu}_1=\hat{\mu}_0+\hat{\gamma}_1x_1$, dove $\hat{\gamma}_1$ è un parametro scelto in modo che il nuovo vettore di residui $\Bar{y}_2-\hat{\mu}_1$ bisechi l'angolo fra $x_1$ e $X_2$.
Poi, il terzo vettore di stima sarà $\hat{\mu}_2=\hat{\mu}_1+\hat{\gamma}_2u_2$, dove $u_2$ è l'unità bisettrice.
Infine, poiché le covariate sono soltanto due, il processo terminerebbe in $\hat{\mu}_2=\Bar{y}_2$.
Pertanto, la stima viene spostata nello spazio lineare in direzione del candidato più correlato, con lunghezza del passo pari a quella sufficiente per rendere il vettore dei residui $\Bar{y}_2-\hat{\mu}$ ugualmente correlato ai candidati presi in considerazione - qui $x_1$ e $x_2$, circostanza che si verifica, appunto, quando il vettore dei residui biseca l'angolo fra di essi:
$$c_1(\hat{\mu})=c_2(\hat{\mu}).$$
La differenza fra la classica \textit{Forward Selection} e il metodo \textit{LARS} dipende proprio dalla scelta dell'ampiezza del passo $\hat{\gamma}$: in questo caso, il primo algoritmo dei due citati compierebbe un passo tale per cui $\hat{\mu}_1=\Bar{y}_1$, ovvero abbastanza lungo per eguagliare la stima alla proiezione di $y$ nello spazio lineare $\mathcal{L}(X_1)$; il secondo, invece, trova una misura intermedia, che non sia eccessivamente avida, né comporti un eccessivo costo computazionale.
A tal proposito, la linea "a scaletta" in Figura \ref{fig:lars-strategy} ipotizza quali potrebbero essere i piccoli spostamenti effettuati dal metodo \textit{Forward Stagewise}.

In Figura \ref{fig:lars-p3}, invece, è possibile osservare un esempio del movimento del vettore delle stime quando $p=3$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{equiang2.png}
    \caption{L'algoritmo \textit{LARS} nel caso in cui il numero di covariate sia $p=3$. La linea blu riproduce il movimento dell'algoritmo \textit{LARS}, mentre la linea verde disegna la bisettrice fra gli assi relativi ai candidati considerati.}
    \label{fig:lars-p3}
\end{figure}

Nell'implementazione, il valore di $\hat{\gamma}$ viene calcolato con la seguente formula \cite{B}:
$$\hat{\gamma}=min^+_{j\in A^c}\{\frac{\hat{C}-\hat{c}_j}{A_A-a_j},\frac{\hat{C}+\hat{c}_j}{A_A+a_j}\}$$
dove
\begin{itemize}
    \item $min^+_{j\in A^c}$ indica la minima componente positiva fra ogni scelta di $j$;
    \item $\hat{C}$ è il massimo nel vettore delle correlazioni $\hat{c}$, di cui $\hat{c}_j$ è il j-esimo valore;
    \item $A_A=(1'_A(X'_AX_A)^{-1}1_A)^{-\frac{1}{2}}$, con $X_A$ sottoinsieme della matrice del disegno originale che considera solo un numero predefinito di colonne e $1_A$ vettore di 1 di lunghezza pari al sottoinsieme di colonne $A$;
    \item $a_j$ è il j-esimo valore del vettore dei prodotti interni $a=X'X_AA_A(X'_AX_A)^{-1}1_A$.
\end{itemize}

Le criticità e limitazioni del metodo \textit{LARS} possono essere riassunte nei seguenti punti:
\begin{enumerate}
    \item dato che la variabile dipendente studiata è disturbata da una quantità ignota di rumore e poiché l'algoritmo viene utilizzato su una matrice del disegno $X$ ad alta dimensionalità e multi-collineare, non esiste una forte motivazione per ipotizzare che le variabili selezionate abbiano un'alta probabilità di corrispondere alle variabili casuali sottostanti. Questo problema non è in realtà legato solo all'implementazione dell'algoritmo LARS, in quanto criticità comune a tutti gli approcci di \textit{Feature Selection} che cercano di estrarre le componenti deterministiche sottostanti. Tuttavia, poiché LARS si basa su una stima iterativa dei residui, appare particolarmente sensibile agli effetti del rumore. \textit{Weisberg}\cite{C} fornisce un esempio empirico basato su una nuova analisi dei dati originariamente utilizzati per convalidare il metodo LARS, dimostrando come la selezione delle variabili si riveli poco efficace in presenza di predittori altamente correlati;
    \item spesso i dati ad alta dimensionalità provenienti dalle applicazioni reali mostrano un certo grado di collinearità in  alcune variabili, per cui il difetto spiegato da \textit{Weisberg} limita di fatto l'utilizzo del \textit{LARS} nelle situazioni in cui $p\gg n$.
\end{enumerate}


Infine, è noto che, grazie alle proprie caratteristiche, \textit{LARS} sia associato ad una semplice approssimazione del $C_p$ di \textit{Mallow} che non necessita di ulteriore computazione oltre a quella già richiesta per la stima del vettore $\hat{\beta}$ \cite{B}:
$$C_p(\hat{\mu})=\frac{\left\lVert y-\hat{\mu} \right\rVert^2}{\sigma^2}-n+2\text{df}_{\mu,\sigma^2}$$
dove $\sigma^2$ è l'errore irriducibile, $n$ il numero di osservazioni e $\text{df}_{\mu,\sigma^2}=\sum_{i=1}^n\frac{\text{Cov}(\hat{\mu}_i,y_i)}{\sigma^2}$ i gradi di libertà dello stimatore $\hat{\mu}$.


\newpage
\section{Simulazione}  % moiraghif
Dopo aver implementato l'algoritmo, si esegue una simulazione col metodo Monte Carlo per scomporre la varianza dei vari modelli e analizzarne meglio le prestazioni in un caso ad alta dimensionalità (\textit{high dimentional}).
Per tutta la simulazione, si opera con matrici di dimensioni ridotte ($30 \times 50$) ma sempre \textit{high dimentional}: in questo modo si alleggerisce il carico di lavoro della macchina, permettendo quindi di effettuare un numero maggiore di iterazioni.
La vera funzione $\phi$ (nota e fissa, essendo un ambiente simulativo) è ottenuta tramite la combinazione lineare di un vettore $\beta$ di lunghezza 50, estratto casualmente da una variabile uniforme, di cui sono stati annullati, in modo casuale, 15 valori: in questo modo si elimina completamente la correlazione tra la variabile risposta e i corrispettivi regressori, aggiungendo informazioni superflue nella matrice del disegno $X$.
In questo caso, sempre \textit{high dimentional} (infatti $50 - 15 = 35 > 30$, il numero di colonne non nulle è comunque maggiore del numero di righe), è necessaria un'operazione di selezione delle \textit{features} da parte del modello.

La matrice del disegno $X$, invece, è generata per ogni simulazione - o addirittura iterazione, nel caso randomico -, estraendo le osservazioni sempre dalla medesima popolazione.
Le colonne della matrice sono estratte da una variabile uniforme in modo indipendente, in modo tale da garantire l'eteroschedasticità e lo stesso ordine di grandezza dei valori\footnote{Si rimanda al codice.}, rispettando pertanto le premesse dei modelli \textit{LARS} e \textit{Ridge}.
Per semplicità di implementazione, il \textit{training set} e il \textit{test set} hanno lo stesso numero di dati, permettendo di usare lo stesso generatore e assicurando che tutte le osservazioni siano estratte dalla medesima popolazione.

Si è deciso di non effettuare alcun tipo di \textit{pre-processing} sui dati, essendo privi di semantica, se non la standardizzazione con media nulla e varianza unitaria - richiesta dal modello LARS.
Inoltre, non si aggiunge la colonna dell'intercetta, in quanto, avendo media nulla, sarebbe priva di significato per il modello \textit{Ridge} e inutile per il modello \textit{LARS}.
Per assicurarsi che entrambi i \textit{set} di dati siano sottoposti allo stesso trattamento, è stata scritta una \textit{funzione di ordine superiore} (\verb|train|) che si occupa di standardizzare la matrice del disegno processata dal modello, e de-standardizzare il vettore dei risultati per riportarlo nella scala dell'originale.

\subsection{Simulazione \textit{Fixed X-Settings}}
In questo tipo di simulazione, partendo dalla stessa matrice dei dati $X$ (fissa per tutta la simulazione), si generano a ogni iterazione il vettore di \textit{training} e il vettore di \textit{test}, che pertanto variano solamente per il rumore gaussiano casuale.
Stimati i parametri del modello sul vettore di \textit{training}, si testa il modello sul vettore di \textit{test}, il tutto per un numero sufficientemente grande di iterazioni, come prevede il sistema Monte Carlo.
Salvati i risultati e conoscendo i valori reali della funzione, si può effettuare la scomposizione della varianza.

Per testare la bontà dell'implementazione, viene misurata la differenza dell'\textit{errore quadratico medio} ($MSE$) teorico - calcolato come somma della varianza e del quadrato della distorsione - e quello ottenuto con la formula
\begin{equation*}
    MSE = \frac{1}{n} \sum_{i=1}^n (y^*_i - \hat{y}_i)^2
\end{equation*}
Durante l'esecuzione, è stato infatti notato che i due valori tendono a convergere, confermando la correttezza dell'algoritmo (come si vede in figura \ref{fig:lars-fixed}).

\begin{figure}[h]
    \centering
    \begin{subfigure}
        \includegraphics[width=0.45\linewidth]{conv_lars_fixed-86.png}
    \end{subfigure}
    \begin{subfigure}
        \includegraphics[width=0.45\linewidth]{bv_lars_fixed-97.png}
    \end{subfigure}
    \caption{Convergenza degli stimatori del LARS ai loro valori teorici.
    A sinistra si nota come l'MSE converga al suo valore teorico già dopo poche iterazioni; a destra si vede l'andamento della varianza (in azzurro) e del quadrato della distorsione (in rosso).}
    \label{fig:lars-fixed}
\end{figure}

Per i modelli \textit{Ridge}, sono stati eseguiti vari test con diversi valori del parametro $\lambda$, senza, tuttavia, porsi l'obiettivo di ottimizzarlo.
I risultati sono riassunti in figura \ref{fig:fixed-total}.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fixed_barplot.png}
    \caption{Andamento della varianza e della distorsione nei vari modelli.
    Si nota come la varianza sia visibile solo nel \textit{LARS}, essendo nel \textit{Ridge} di qualche ordine di grandezza minore rispetto alla distorsione.}
    \label{fig:fixed-total}
\end{figure}

Si nota come lo stimatore \textit{Ridge} abbia una varianza tendente a $0$, con valori della distorsione che crescono - tendenzialmente - col parametro $\lambda$.
Questo può essere spiegato dal fatto che l'algoritmo \textit{LARS} calcola in modo iterativo le stime del vettore $\beta$, effettuando una selezione delle più significative ma includendone un numero minore rispetto a quelle della funzione reale $\phi$, risultando pertanto distorto e molto variabile rispetto alla selezione.
Al contrario, l'algoritmo \textit{Ridge} tenta di ottimizzare lo scarto quadratico medio delle stime - con penalità $\lambda$ - considerando l'intera matrice, e dunque tutte le variabili significative.
Per questa ragione, i valori dello stimatore \textit{LARS} sono maggiormente influenzati dalle oscillazioni casuali del rumore, che rende una variabile più o meno correlata rispetto ad un'altra, influenzando quindi il processo di selezione. Invece, \textit{Ridge} considera tutte le covariate indipendentemente dalla correlazione con la variabile risposta.
Sperimentando con la quantità di valori nulli del parametro $\beta$ della funzione $\phi$, si nota infatti che l'errore totale del modello \textit{LARS} cresce col diminuire delle variabili significative - mostrando di dipendere molto dal rumore -, ma diventa paragonabile al \textit{Ridge} - con $\lambda$ elevati - quando tutte le variabili sono significative.
La forte varianza delle stime può essere spiegata dal fatto che il numero di osservazioni della matrice del disegno non è così alta da permettere una buona stima dell'errore irriducibile $\sigma$, e della vera correlazione $\varrho$ tra i predittori e la variabile risposta.

Tuttavia, avendo mantenuto inalterati i valori della matrice del disegno $X$ per ciascuna iterazione, non è possibile fare supposizioni sulla capacità di generalizzazione dei modelli analizzati.
Viene eseguita, pertanto, una simulazione \textit{Random X-Settings}, in modo tale da ottenere dei risultati più accurati.

\subsection{Simulazione \textit{Random X-Settings}}
In questo tipo di simulazione, i dati sono generati a ogni iterazione. Tuttavia, per semplificare l'implementazione, si è deciso di tenere fisso il \textit{test set}, in modo tale da rendere più efficiente il calcolo degli indici mediante operazioni matriciali, garantendo comunque una buona mappatura dello spazio della funzione stimata $\hat{f}$.
Questa precauzione non incide sull'efficacia del sistema: infatti, caratteristica fondamentale della simulazione \textit{Random X-Settings} è che il \textit{training set} e il \textit{test set} siano estrazioni stocastiche e indipendenti della stessa popolazione.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{random_barplot.png}
    \caption{Nella simulazione randomica, a differenza del caso precedente, la varianza è più marcata: si testa infatti la capacità di generalizzazione del modello in punti dello spazio diversi da quelli usati per l'allenamento.}
    \label{fig:random-total}
\end{figure}
A differenza della simulazione precedente, si nota un aumento della varianza totale del modello. Questo è facilmente intuibile, dato che si aggiunge casualità nella scelta delle osservazioni.
Per l'algoritmo \textit{Ridge}, si nota che la distorsione delle stime diminuisce all'aumentare del parametro $\lambda$, mentre la varianza - tendenzialmente - aumenta.
A differenza del caso precedente, tuttavia, si nota che, al diminuire delle variabili significative del vero vettore $\beta$, l'algoritmo \textit{Ridge} produce risultati migliori (come mostrato in figura \ref{fig:lars-low-dim}) data la sua selezione iterativa delle variabili significative.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{random_barplot_low.png}
    \caption{Andamento dell'errore nei vari modelli con matrice di input $\underset{30 \times 50}{X}$, con 25 variabili significative.}
    \label{fig:lars-low-dim}
\end{figure}

Dunque, si può concludere che l'algoritmo \textit{LARS}, a confronto col metodo di regressione \textit{Ridge}, offre stime estremamente variabili, ma effettua una selezione delle \textit{features} significative che può essere molto utile in alcuni contesti \textit{high dimentional}.


\newpage
\section{Applicazione pratica}  % AngusFangus
In questa sezione, l'algoritmo \textit{LARS} e la regressione \textit{Ridge} sono stati applicati ad un caso di studio, utilizzando dei dati reali, con lo scopo di analizzarne il comportamento in uno scenario non controllato.
Il \textit{dataset}\cite{F} preso in considerazione raccoglie dati provenienti da uno studio integrato sull'RNA messaggero (\textit{mRNA}), micro RNA (\textit{miRNA}): delle $946$ variabili, $939$ si riferiscono a dati genomici, mentre le altre $5$ a variabili cliniche (quali età e tipologia di tumore).
I campioni di tessuto sono stati raccolti a partire da una coorte di $123$ pazienti, sottoposti a resezione chirurgica presso un istituto francese\footnote{Istituto \textit{Mutualiste Montsouris}, Parigi, Francia.} tra il 30 gennaio 2002 e il 26 giugno 2006 \cite{E}.
Lo scopo della ricerca è quello di prevedere il tempo di sopravvivenza a seguito di una terapia sperimentale in funzione dei nucleotidi dell'mRNA, per poter effettuare test mirati solamente sui marcatori significativi.
La variabile risposta $y$ è relativa al tempo di sopravvivenza post trattamento, mentre la restante colonna \textit{Delta} indica se il dato sia stato censurato, ovvero se il periodo di osservazione sia stato interrotto senza informazioni in merito all'evento di interesse.

Data la scarsa qualità dei dati \textit{censurati} (ovvero il cui tempo di sopravvivenza non è noto ma solamente supposto), si è deciso di scartare le osservazioni, riducendo il numero di pazienti a $64$.
Inoltre il tempo di sopravvivenza medio in caso di dato censurato è nettamente inferiore: $\mathbb{E}[y | Delta = 1] = 1.68$ (dato censurato), mentre $\mathbb{E}[y | Delta = 0] = 4.69$, rendendo necessaria tale scelta per non introdurre \textit{bias} nei dati.

Le correlazioni tra i regressori e la variabile risposta hanno un valore assoluto massimo del $35\%$, con alcune variabili fortemente correlate tra di loro, anche oltre al $90\%$: si è dunque in un caso di multicollinearità.
Nel correlogramma (figura \ref{fig:corrplot}) sono riassunte le precedenti valutazioni, riportando solo alcune delle variabili per esigenze di visualizzazione.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{corrplot.png}
    \caption{Correlogramma delle variabili maggiormente legate alla $y$ e tra loro.}
    \label{fig:corrplot}
\end{figure}
Il predittore clinico $Type$ è di tipo categorico, ed assume 4 differenti modalità, in riferimento alla tipologia di tumore. Per questo motivo necessita di essere rimpiazzato da altrettante variabili indicatore.

In definitiva, il \textit{dataset} effettivamente preso in esame è composto da $64$ osservazioni e $947$ regressori: rispetta, perciò, la principale caratteristica dell'\textit{high dimentional data}, $p \gg n$.
Inizialmente, le unità statistiche vengono suddivise casualmente in un \textit{training set} pari al $67\%$ del loro totale e un \textit{test set} del $33\%$. In un secondo momento, però, si è scelto di effettuare la separazione tramite \textit{Cross-validation}, per ridurre il rischio di \textit{overfitting} e altri problemi legati alla \textit{Selection bias}.


\subsection{Analisi del caso reale con regressione \textit{Ridge}}
L'obiettivo è analizzare l'andamento dell'errore di previsione in rapporto al variare del parametro di \textit{tuning} $\lambda$, ponendo particolare attenzione alla compressione verso lo $0$ subita dai coefficienti di regressione.
La matrice del disegno $X$ e il vettore delle risposte $y$ del \textit{training set} vengono standardizzati, con media nulla e varianza unitaria, evitando di far dipendere la regolarizzazione dall'ordine di grandezza, dall'unità di misura con cui i dati sono espressi e dalla loro variabilità.
Anche il \textit{test set} è standardizzato sulla base dei dati di \textit{train}: proveniendo dalla stessa distribuzione ed essendo divisi in modo casuale, si suppone che i due \textit{set} siano equi-distribuiti.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{ridge_mse.png}
    \caption{Andamento dell'errore quadratico medio nella regressione \textit{Ridge} nella prima suddivisione \textit{train-test}.}
    \label{fig:ridge_mse}
\end{figure}
Si stima il modello \textit{Ridge} sul \textit{train set} per una serie di $\lambda$ e si misurano le performance sul \textit{test set}.
Il valore dell'$MSE$ in funzione di $\lambda$ presenta un andamento dapprima decrescente, e in seguito, raggiunto il minimo, crescente fino a raggiungere un asintoto, posto su un livello nettamente minore rispetto al valore assunto per $\lambda \to 0$  \footnote{Ad esempio, il valore dell'$MSE$ per $\lambda = 0.001$ è $3.24$, mentre, per $\lambda = 10^9$, è $1.96$.}, come mostrato in figura \ref{fig:ridge_mse}.
%L'errore quadratico medio risulta minore per $\lambda$ molto elevati\footnote{Ad esempio, il valore dell'MSE per $\lambda = 7800$ è $1.918$, nel caso della prima divisione \textit{train-test}, figura \ref{fig:ridge_mse}}, e, anche incrementando il parametro esponenzialmente, esso non supera mai il valore assunto in corrispondenza del $\lambda$ più piccolo\footnote{Il valore dell'MSE per $\lambda = 0.001$ è $3.24$, mentre, per $\lambda = 10^9$, è $1.96$.}.
%I coefficienti, all'aumentare del parametro $\lambda$, tendono a $0$, con un andamento non monotono.
% Inoltre, come illustrato nella sezione \ref{ridge}, essi non potranno mai essere esattamente nulli, poiché la regolarizzazione \textit{Ridge} non è in grado di selezionare le covariate. 
%Ciò fa supporre che i veri valori dei regressori $\beta$ - non noti, poiché in uno scenario non controllato - siano molto bassi - il \textit{Ridge} performa in questi casi in modo efficace - e che 
%La distorsione del modello sia moderata - all'aumentare del parametro di \textit{tuning} cresce il quadrato della distorsione e diminuisce la varianza.
Questo comportamento si ottiene anche con la \textit{Cross-validation} (con $K=10$), come mostrato in figura \ref{fig:ridge_lambda}.

La scelta del modello migliore può basarsi sul valore di $\lambda$ che minimizza l'errore quadratico medio ($MSE$).
In alternativa, può essere conveniente adottare l'approccio \textit{one standard error rule}, da prediligere se si cerca di interpretare l'output per fare inferenza, e che dà origine ad un modello più parsimonioso nella seguente maniera: 
\begin{itemize}
    \item si seleziona il parametro di \textit{tuning} in corrispondenza di cui l'errore quadratico medio è minimo;
    \item si costruisce una banda di confidenza pari a una volta la deviazione standard dell'errore, con formula $[\Bar{x} - \hat{\sigma}_x; \Bar{x} + \hat{\sigma}_x]$;
    \item il modello selezionato è quello più parsimonioso il cui valore dell'$MSE$ rientra nella banda di confidenza.
\end{itemize}
Poiché il metodo \textit{Ridge} non è in grado di effettuare una vera \textit{feature selection}, bensì solo di comprimere i valori dei coefficienti verso $0$, il modello selezionato dalla \textit{one standard error rule} è quello in corrispondenza del $\lambda$ maggiore all'interno della banda di confidenza, poiché il vettore $\hat{\beta}^\lambda$ dei suoi coefficienti è maggiormente penalizzato.

% Nel caso \textit{Ridge} ciò avviene non eliminando le variabili meno significative - in quanto non è effettuata una \textit{feature selection} - bensì comprimendo il vettore $\hat{\beta}^\lambda$ verso lo $0$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{ridge_oserule.jpeg}
    \caption{scelta del valore di $\lambda$ ottimo attraverso la \textit{cross-validation}.}
    \label{fig:ridge_lambda}
\end{figure}
Se si utilizza la \textit{Cross-validation} per la divisione \textit{train-test} con $K=10$ e  $\lambda$ \textit{one standard error rule}, mediando su tutte le cartelle, si ottiene un errore quadratico medio di $2.67$ con il parametro di \textit{tuning} $\lambda = 584$.

\subsection{Analisi del caso reale con algoritmo \textit{LARS}}
In seguito, la soluzione \textit{Ridge} è stata confrontata con quella prodotta dall'algoritmo \textit{LARS}, che, grazie alla capacità di eseguire una vera \textit{feature selection}, può portare a risultati interessanti per un esperto del dominio.
Oltre a prestare attenzione all'errore quadratico medio sul \textit{test set} al termine dell'esecuzione dell’algoritmo, si osservano le variazioni subite dalla stima dell'\textit{MSE} ad ogni iterazione e le variabili selezionate dal modello.
L'algoritmo \textit{LARS}, nel caso specifico, effettua $n - 1$ iterazioni, selezionando in ciascuna al più una variabile.

Con la medesima suddivisione iniziale \textit{train-test} utilizzata con la regolarizzazione \textit{Ridge}, si ottiene un errore quadratico medio pari a $3.42$; i regressori scelti sono 34, soltanto uno relativo ad una misura clinica e i restanti al \textit{miRNA}.
Ad ogni passo dell'algoritmo, il comportamento è piuttosto singolare: per ogni variabile entrante, l'\textit{MSE} cresce, registrando il valore più basso ($1.96$) con una singola \textit{feature}. In figura \ref{fig:lars_cpath} viene mostrato il \textit{coefficient path} delle 10 variabili più significative - ovvero, essendo standardizzate, con valore assoluto dei coefficienti più elevato.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{lars_cpath.png}
    \caption{\textit{Coefficient path} delle 10 variabili più significative in funzione delle iterazioni dell'algoritmo $LARS$, usando il $67\%$ delle osservazioni per stimare i parametri.}
    \label{fig:lars_cpath}
\end{figure}

Successivamente, l'algoritmo \textit{LARS} viene testato mediante suddivisione con \textit{Cross-validation}.
Tuttavia, data la maggiore complessità computazionale dell'algoritmo, viene scelto un numero di iterazioni $K=5$, minore rispetto al modello \textit{Ridge}.
Anche in questo caso, l'errore minimo è trovato in corrispondenza del modello composto da una sola \textit{feature}, il quale coincide con quello selezionato dalla \textit{one standard error rule} (figura \ref{fig:lars_kcv}, la linea verticale identifica questa sovrapposizione).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{lars_kcv.png}
    \caption{Andamento dell'errore quadratico medio per ogni iterazione dell'algoritmo LARS con \textit{cross-validation}. Le linee tratteggiate identificano le bande di confidenza date dallo \textit{standard error}.}
    \label{fig:lars_kcv}
\end{figure}

Questo comportamento è da ricondurre all'alta correlazione tra le variabili esplicative, emersa nelle fasi preliminari dell'analisi. Come evidenziato nella sezione 1, la principale criticità del \textit{LARS} consiste nell'elevata dipendenza dall'errore casuale e dalla collinearità dei regressori.

\label{Weisberg}
Proprio per questo motivo, Weisberg critica l'assunto secondo cui l'algoritmo \textit{LARS} possa essere ritenuto superiore ai metodi di regressione standard\cite{C}, dimostrando quanto esso possa, in caso di matrice del disegno multicollineare, produrre risultati bizzarri, soprattutto in caso di alta dimensionalità.
La sua dimostrazione empirica parte dall'aggiunta di $N$ nuovi predittori a un dataset reale composto da $N$ variabili esplicative numeriche, creati moltiplicando ciascun vettore originale a una costante e arrotondando i valori - per aggiungere del rumore.
Ci si aspetterebbe, dunque, che nessuna delle nuove covariate venga considerata come regressore attivo.
Eppure, utilizzando la stessa funzione fornita dagli autori del metodo, l'algoritmo \textit{LARS} seleziona un numero di colonne identico rispetto a quanto ottenuto dall'applicazione sul dataset originale, ma includendo due delle nuove variabili arrotondate.
Viene anche notato che, assieme a queste due covariate, il modello finale comprende i rispettivi vettori da cui esse sono state generate, ipotizzando - erroneamente - che il rumore derivato dall'arrotondamento sia significativo.

L'inclusione ed esclusione di elementi nel modello finale dipende dalla distribuzione marginale di $X$ tanto quanto dalla distribuzione condizionata $y|X$. In un caso reale, anche se la collinearità fra i candidati e la quantità ignota di rumore non sono in grado di modificare la distribuzione di $y|X$, possono facilmente influenzare la distribuzione marginale di $X$, facendo variare il set di predittori attivi selezionati da \textit{LARS} o qualsiasi altro metodo basato sulla correlazione.

Detto ciò, come indicato nella prima sezione, nel caso in cui la matrice del disegno abbia dimensionalità molto elevata, è ragionevole supporre che essa sia influenzata da una grande quantità di rumore ignoto. Pertanto, nello studio presentato in questa sezione, il difetto intrinseco dell'algoritmo viene amplificato dalla grandissima numerosità delle colonne e dalla loro collinearità, finendo per offrire, appunto, risultati poco affidabili.

Con lo scopo di verificare l'effetto nel caso di matrice del disegno non strettamente correlata, è stata implementata una funzione che forza l'indipendenza delle variabili. Si osserva come l'andamento dell'errore non sia più crescente, bensì vari a seconda del numero di regressori selezionati.  
I risultati ottenuti dai differenti modelli sono riassunti nella tabella \ref{tab:risultati}.
\begin{table}[h!]
  \centering
    \begin{tabular}{lccc}
    Modello                 & $MSE$ & $\tilde{R}^2$ & Variabili \\
    \hline
    Ridge                   & 2.67  & 1.04          & 947 \\
    Lars                    & 5.37  & 3.46          & 108 \\
    Lars ($X$ trasformata)  & 1.94  & -0.49          & 1 \\
    \hline
  \end{tabular}
  \caption{Risultati ottenuti dai modelli (con la matrice del disegno normalizzata e applicando una trasformazione lineare che renda indipendenti i regressori). Gli indici di bontà sono calcolati tramite il sistema di \textit{Cross Validation}, mentre il numero di variabili significative è ottenuto stimando i coefficienti con tutte le osservazioni (per permettere all'algoritmo LARS di aumentare l'insieme di variabili attive).}
  \label{tab:risultati}
\end{table}

\newpage

\section{Conclusioni}
Nel presente Progetto, sono state confrontate due variazioni della regressione lineare (i modelli \textit{LARS} e \textit{Ridge}) in casi \textit{High Dimentional} sia reali sia simulativi. Caratteristica che rende interessante l'uso del modello \textit{LARS} è la sua capacità di effettuare \textit{feature selection}, riducendo quindi il numero di variabili di interesse per un esperto di dominio.
Tuttavia, nell'ambiente simulativo, si è notato che i risultati del modello \textit{LARS}, rispetto al \textit{Ridge}, erano particolarmente influenzati dal rumore casuale, ignorando persino la non significatività di alcune \textit{features}. Nell'ambiente reale, invece, i risultati erano distorti dalla correlazione tra i singoli regressori.
Di fatto, dunque, nonostante sia di particolare interesse teorico, questo sistema non offre prestazioni migliori di quelle offerte dal \textit{Ridge}: come espresso da Weisberg, già citato nella sezione \ref{Weisberg}, l'algoritmo non offre risultati migliori, in casi di multicollinearità, rispetto alla regressione lineare standard.

L'unico caso in cui convenga applicare l'algoritmo \textit{LARS}, rispetto ad altri sistemi di regressione lineare, è quello in cui la correttezza della previsione sia di importanza marginale rispetto allo studio inferenziale dell'esperto di dominio, quindi in ambienti di ricerca.
Infatti un esperto di dominio potrebbe essere più interessato alla selezione delle variabili e al loro significato semantico rispetto ai risultati ottenuti.


\newpage
\section*{Codice}
L'intero codice, implementato col linguaggio \verb|R|, è disponibile al link: \url{https://github.com/moiraghif/LARS}.

\begin{thebibliography}{9}

\bibitem{A} 
Weisberg S., \textit{Applied Linear Regression}, Wiley, New York, 1980.
 
\bibitem{B} 
\href{http://statweb.stanford.edu/~imj/WEBLIST/2004/LarsAnnStat04.pdf}{Efron B., Hastie T., Johnstone I., Tibshirani R., \textit{Least Angle Regression}, Stanford University, Stanford, 2003.}


\bibitem{C} 
\href{https://arxiv.org/pdf/math/0406473.pdf}{Discussione di Weisberg, su Efron, B; Hastie, T; Johnstone, I; Tibshirani, R., \textit{"Least Angle Regression"}, nella rivista \textit{"Annals of Statistics."}, 32 (2): pp. 407–499.} 

\bibitem{D} 
Hastie, T; Tibshirani, R.; Friedman J.; \textit{"The Elements of Statistical Learning: Data Mining, Inference, and Prediction"}, Stanford University, Stanford, 2009. 

\bibitem{E}
\href{https://www.researchgate.net/publication/259152395_Integrated_molecular_portrait_of_non-small_cell_lung_cancers}{Lazar V. et al. \textit{Integrated molecular portrait of non-small cell lung cancers.}, BMCMedical Genomics 6:53-65 , 2013.}

\bibitem{F}
\href{https://github.com/jedazard/PRIMsrc}{J-E. Dazard et al. \textit{R package PRIMsrc: Bump Hunting by Patient Rule Induction Method for Survival, Regression and Classification}, JSM Proceeding, 2015}

\end{thebibliography}

\end{document}
